# event-extraction


"[GoogleNews-vectors-negative300](https://code.google.com/archive/p/word2vec/)" is used as pre-trained word2vec model.

## TODO 

- Increase the number of data used for evaluation (currently only 30)
- Add word2vec

```bash
2018-11-10T03:38:16.408551: loss 0.00862374, acc 1
2018-11-10T03:38:16.484978: loss 0.427776, acc 0.933333
2018-11-10T03:38:16.556502: loss 0.552501, acc 0.9
2018-11-10T03:38:16.630882: loss 0.290723, acc 0.9
2018-11-10T03:38:16.699259: loss 0.120635, acc 0.966667
2018-11-10T03:38:16.770981: loss 0.172747, acc 0.966667
2018-11-10T03:38:16.845399: loss 0.0313517, acc 0.966667
2018-11-10T03:38:16.912906: loss 0.238546, acc 0.966667
2018-11-10T03:38:16.984497: loss 0.0615598, acc 0.966667
2018-11-10T03:38:17.052549: loss 0.120855, acc 0.966667
2018-11-10T03:38:17.120559: loss 0.213745, acc 0.9
2018-11-10T03:38:17.196295: loss 0.0645926, acc 0.966667
2018-11-10T03:38:17.268672: loss 0.0221864, acc 1
2018-11-10T03:38:17.341155: loss 0.0375219, acc 1
2018-11-10T03:38:17.411111: loss 0.0501572, acc 0.966667
2018-11-10T03:38:17.484186: loss 0.986013, acc 0.9
2018-11-10T03:38:17.554882: loss 1.06596, acc 0.9
2018-11-10T03:38:17.626529: loss 0.120261, acc 0.933333
2018-11-10T03:38:17.699312: loss 1.08561, acc 0.9
2018-11-10T03:38:17.768334: loss 0.0488479, acc 1
2018-11-10T03:38:17.844030: loss 0.00877127, acc 1
2018-11-10T03:38:17.913705: loss 0.0676153, acc 0.966667
2018-11-10T03:38:17.981607: loss 0.652455, acc 0.9
2018-11-10T03:38:18.054250: loss 0.156669, acc 0.966667
2018-11-10T03:38:18.125788: loss 0.288849, acc 0.933333
2018-11-10T03:38:18.199569: loss 0.133604, acc 0.966667
2018-11-10T03:38:18.276992: loss 0.151481, acc 0.966667
2018-11-10T03:38:18.352524: loss 0.439608, acc 0.933333
2018-11-10T03:38:18.423659: loss 0.178131, acc 0.9
2018-11-10T03:38:18.496560: loss 0.286932, acc 0.866667
2018-11-10T03:38:18.569519: loss 0.0904374, acc 0.933333
2018-11-10T03:38:18.642055: loss 0.0318502, acc 1
2018-11-10T03:38:18.712787: loss 0.164931, acc 0.933333
2018-11-10T03:38:18.784207: loss 0.393558, acc 0.9
2018-11-10T03:38:18.854141: loss 0.625278, acc 0.9
2018-11-10T03:38:18.925009: loss 1.23597, acc 0.833333
2018-11-10T03:38:18.996662: loss 0.19571, acc 0.933333
2018-11-10T03:38:19.068105: loss 0.0390664, acc 1
2018-11-10T03:38:19.142528: loss 0.054021, acc 0.966667
2018-11-10T03:38:19.212596: loss 0.137261, acc 0.933333
2018-11-10T03:38:19.285346: loss 0.27786, acc 0.9
----test results---------------------------------------------------------------------
eval accuracy:0.7666666507720947
input_y :  [7, 7, 3, 7, 12, 7, 7, 7, 7, 7, 7, 7, 8, 33, 7, 7, 18, 29, 7, 7, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7]
predicts : [ 7  7  7  7 12  7  7  7  7  7  7  7 18 24  7  7  8  7  7  5  7  7  7  7 7  7  7  7  7  7]
              precision    recall  f1-score   support

           3       0.00      0.00      0.00         1
           5       0.00      0.00      0.00         1
           7       0.88      0.96      0.92        23
           8       0.00      0.00      0.00         1
          12       1.00      1.00      1.00         1
          18       0.00      0.00      0.00         1
          24       0.00      0.00      0.00         0
          29       0.00      0.00      0.00         1
          33       0.00      0.00      0.00         1

   micro avg       0.77      0.77      0.77        30
   macro avg       0.21      0.22      0.21        30
weighted avg       0.71      0.77      0.74        30
```
